{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering for job segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train_data.csv\")\n",
    "df_test = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train = df_train[\"content\"]\n",
    "    y_train = df_train[\"label\"]\n",
    "\n",
    "    x_test = df_test[\"content\"]\n",
    "    y_test = df_test[\"label\"]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_regex = re.compile(r\"[\\w.-]+@[\\w.-]+\")\n",
    "url_regex = re.compile(r\"(http|www)[^\\s]+\")\n",
    "date_regex = re.compile(r\"[\\d]{2,4}[ -/:]*[\\d]{2,4}([ -/:]*[\\d]{2,4})?\") # a way to match date\n",
    "\n",
    "def clean_special_patterns(text):\n",
    "    \"\"\"Remove special patterns - email, url, date etc.\"\"\"\n",
    "    text = url_regex.sub(\"\", text)\n",
    "    text = email_regex.sub(\"\", text)\n",
    "    text = date_regex.sub(\"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = x_train.apply(clean_special_patterns)\n",
    "test_corpus = x_test.apply(clean_special_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "Vec = vectorizer.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Vec.transform(train_corpus)\n",
    "X_test = Vec.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_size = 50\n",
    "svd = TruncatedSVD(\n",
    "  n_components=n_size,\n",
    "  n_iter=10,\n",
    "  random_state=42\n",
    "  )\n",
    "\n",
    "svd_fit = svd.fit(X_train)\n",
    "X_train = svd_fit.transform(X_train)\n",
    "X_test = svd_fit.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = [\"svd_\"+str(i) for i in range(n_size)]\n",
    "df_svd_train = pd.DataFrame(X_train, columns=feature_name)\n",
    "df_svd_test = pd.DataFrame(X_test, columns=feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat features\n",
    "- Text-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "def count_uniquewords(text):\n",
    "    return len(set(str(text).split()))\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(str(text))\n",
    "\n",
    "def word_density(text):\n",
    "    return count_chars(text) / (count_words(text) + 1)\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stopwords = [word for word in str(text).split() if word in stop_words]\n",
    "    return len(stopwords)\n",
    "\n",
    "def count_puncts(text):\n",
    "    puncts = re.findall('[' + punctuation + ']', str(text))\n",
    "    return len(puncts)\n",
    "\n",
    "def count_upperwords(text):\n",
    "    upperwords = re.findall(r\"\\b[A-Z0-9]+\\b\", str(text))\n",
    "    return len(upperwords)\n",
    "\n",
    "def count_firstwords(text):\n",
    "    \"\"\"count first word of sentence\"\"\"\n",
    "    firstwords = re.findall(r\"\\b[A-Z][a-z]+\\b\", str(text))\n",
    "    return len(firstwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(text_series):\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    df_features[\"word_count\"] = text_series.apply(count_words)\n",
    "    df_features[\"uniqueword_count\"] = text_series.apply(count_uniquewords)\n",
    "    df_features[\"char_count\"] = text_series.apply(count_chars)\n",
    "    df_features[\"word_density\"] = text_series.apply(word_density)\n",
    "    df_features[\"stopword_count\"] = text_series.apply(count_stopwords)\n",
    "    df_features[\"punct_count\"] = text_series.apply(count_puncts)\n",
    "    df_features[\"upperword_count\"] = text_series.apply(count_upperwords)\n",
    "    df_features[\"firstword_count\"] = text_series.apply(count_firstwords)\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_train = make_features(train_corpus)\n",
    "df_text_test = make_features(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_dic = {\n",
    "    \"NN\" : \"noun\", \"NNS\" : \"noun\", \"NNP\": \"noun\", \"NNPS\" : \"noun\",\n",
    "    \"PRP\" : \"pron\", \"PRP$\" : \"pron\", \"WP\" : \"pron\", \"WP$\" : \"pron\",\n",
    "    \"VB\" : \"verb\", \"VBD\" : \"verb\", \"VBG\" : \"verb\", \"VBN\" : \"verb\", \"VBP\" : \"verb\", \"VBZ\": \"verb\",\n",
    "    \"JJ\" : \"adj\", \"JJR\" : \"adj\", \"JJS\" : \"adj\",\n",
    "    \"RB\"  : \"adv\", \"RBR\" : \"adv\", \"RBS\" : \"adv\", \"WRB\" : \"adj\"\n",
    "}\n",
    "\n",
    "def count_tag(text):\n",
    "    pos_counts = {\n",
    "        \"noun\": 0, \"pron\": 0, \"verb\": 0, \"adj\": 0, \"adv\": 0\n",
    "    }\n",
    "    for w, p in pos_tag(str(text).split()):\n",
    "        try:\n",
    "            tag = pos_dic[p]\n",
    "            pos_counts[tag] = pos_counts[tag] + 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict_train = train_corpus.apply(count_tag)\n",
    "df_pos_train = pd.DataFrame(list(pos_dict_train))\n",
    "\n",
    "pos_dict_test = test_corpus.apply(count_tag)\n",
    "df_pos_test = pd.DataFrame(list(pos_dict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train = pd.concat([df_text_train, df_pos_train], axis=1)\n",
    "df_features_test = pd.concat([df_text_test, df_pos_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge all features and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_out = pd.concat([df_svd_train, df_features_train], axis=1)\n",
    "df_test_out = pd.concat([df_svd_test, df_features_test], axis=1)\n",
    "\n",
    "df_train_out[\"label\"] = y_train\n",
    "df_test_out[\"label\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_out.to_csv(\"train_data.csv\")\n",
    "df_test_out.to_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "stanford_dir = \"/home/ruihaoqiu/stanford-ner-2018-10-16/\"\n",
    "jarfile = stanford_dir + 'stanford-ner.jar'\n",
    "modelfile = stanford_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=modelfile, path_to_jar=jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ner(text):\n",
    "    ner_counts = dict()\n",
    "    ners = st.tag(str(text).split())\n",
    "    print(ners)\n",
    "    for _, p in ners:\n",
    "        if p in ner_counts:\n",
    "            ner_counts[p] = ner_counts[p] + 1\n",
    "        else:\n",
    "            ner_counts[p] = 1\n",
    "    return ner_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Christian is living in Berlin and working at BMW and google, but Amazon is a nice company\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
