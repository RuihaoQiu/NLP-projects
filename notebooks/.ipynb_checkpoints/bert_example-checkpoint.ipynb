{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RuihaoQiu/NLP-projects/blob/master/bert_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5rXQSdpnn_L"
   },
   "source": [
    "## An example of BERT\n",
    "ref \n",
    "1. https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "2. https://www.kdnuggets.com/2020/02/intent-recognition-bert-keras-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0jw2JOPn3aD"
   },
   "outputs": [],
   "source": [
    "## use bert for tensorflow 2\n",
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2m2IJ1sbmT41"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlTpJI_-n7Ui"
   },
   "outputs": [],
   "source": [
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QyMu8mpu4r8"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDO2Uk6_oMLC"
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EE-nxV1ntaYE"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tD-0KIctdTL"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSUUH4ZUti-L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "na5evz7tu-fH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zQF0Mhx6x7o"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_set.csv\")\n",
    "test = pd.read_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUvCgQrzfa93"
   },
   "source": [
    "### Load bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLZn4DLAu-oT"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnKiNB9LfgCz"
   },
   "outputs": [],
   "source": [
    "bert_folder = \"uncased_L-12_H-768_A-12/\"\n",
    "bert_config_file = os.path.join(bert_folder, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(bert_folder, \"bert_model.ckpt\")\n",
    "bert_vocab_file = os.path.join(bert_folder, \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_hevia1vA8c"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-FvJxZMf5Bj"
   },
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=bert_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IM3BK4U7zFoO"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wswibNNt3Rx6"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")\n",
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anb5XlWZ-Apg"
   },
   "outputs": [],
   "source": [
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1CGvH5JJASh"
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"company\" : 0,\n",
    "    \"tasks\": 1,\n",
    "    \"profile\": 2,\n",
    "    \"benefits\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uH4uZJ_v5NUd"
   },
   "outputs": [],
   "source": [
    "def _convert_single(input_text):\n",
    "  tokens = tokenizer.tokenize(input_text)\n",
    "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  return token_ids\n",
    "\n",
    "def _convert_multiple(input_list):\n",
    "  token_ids_list = []\n",
    "  max_len = 1\n",
    "  for sent in tqdm(input_list):\n",
    "    token_ids = _convert_single(sent)\n",
    "    token_ids_list.append(token_ids)\n",
    "  return token_ids_list\n",
    "\n",
    "def _pad(token_ids_list):\n",
    "  x_padded = []\n",
    "  for input_ids in token_ids_list:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_len - len(input_ids))\n",
    "    x_padded.append(np.array(input_ids))\n",
    "  return np.array(x_padded)\n",
    "\n",
    "def convert(input_list):\n",
    "  token_ids_list = _convert_multiple(input_list)\n",
    "  out_array = _pad(token_ids_list)\n",
    "  return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYHEye2I6xyF"
   },
   "outputs": [],
   "source": [
    "X_train = convert(train.sentence)\n",
    "X_test = convert(test.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uska8UwHARVS"
   },
   "outputs": [],
   "source": [
    "y_train = train.label.map(label_dict).values\n",
    "y_test = test.label.map(label_dict).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_D1y9aXJdJG"
   },
   "outputs": [],
   "source": [
    "classes = np.array([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNDOC8bRF5Eg"
   },
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRGa0uulC1nc"
   },
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oljAqxKPHGzq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGC4xDsvC0PV"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = None\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "  input_ids = tf.keras.layers.Input(\n",
    "    shape=(max_len, ),\n",
    "    dtype='int32',\n",
    "    name=\"input_ids\"\n",
    "  )\n",
    "  bert_output = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "  cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "  cls_out = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = tf.keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "  logits = tf.keras.layers.Dropout(0.5)(logits)\n",
    "  logits = tf.keras.layers.Dense(\n",
    "    units=len(classes),\n",
    "    activation=\"softmax\"\n",
    "  )(logits)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_len))\n",
    "\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPo-Z2LC3v66"
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dATelH3G3nZT"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kf-aZlvSPBG"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeY3xhE4TMJ6"
   },
   "outputs": [],
   "source": [
    "log_dir = \"log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "  x=X_train, \n",
    "  y=y_train,\n",
    "  validation_split=0.2,\n",
    "  batch_size=32,\n",
    "  shuffle=True,\n",
    "  epochs=10,\n",
    "  callbacks=[tensorboard_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPRAHmeQMopqxsi2yWA+fWd",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "bert-example.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
